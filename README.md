# chatbot_ollama
An intelligent, conversational chatbot built using **LangChain**, **Streamlit**, and **FAISS**, powered by **Ollama** â€” a local LLM runtime for open-source models. 

This project demonstrates how to combine retrieval-augmented generation (RAG) and custom prompt workflows to build a fully local, privacy-friendly AI assistant.

---

## ğŸš€ Features

- ğŸ’¬ **Conversational Chatbot** â€” Natural, context-aware responses from open-source LLMs.  
- ğŸ§© **Retrieval-Augmented Generation (RAG)** â€” Enhances LLM performance using FAISS-based vector search over local knowledge bases.  
- âš¡ **LangChain Integration** â€” Modular prompt templates, chaining, and retrieval pipeline.  
- ğŸŒ **Streamlit Frontend** â€” Simple and interactive web UI for chatting and visualizing responses.  
- ğŸ›¡ï¸ **Local Inference** â€” Runs entirely on your system with Ollama; no external API keys required.  
- ğŸ” **Embeddings Support** â€” Custom document embeddings for knowledge-based question answering.  

---

## ğŸ—ï¸ Tech Stack

- **Programming Language:** Python 3.11+  
- **Frameworks & Libraries:**  
  - [LangChain](https://www.langchain.com/) â€“ chaining and RAG pipeline  
  - [Streamlit](https://streamlit.io/) â€“ UI interface  
  - [FAISS](https://faiss.ai/) â€“ vector similarity search  
  - [Ollama](https://ollama.ai/) â€“ open-source LLM runtime  
  - [SentenceTransformers](https://www.sbert.net/) â€“ embedding generation  

---

## âš™ï¸ Installation
### 1ï¸âƒ£ Clone this repository
```bash
git clone https://github.com/murugan-bala/chatbot_ollama.git
cd chatbot_ollama

2ï¸âƒ£ Create a virtual environment
python -m venv venv
source venv/bin/activate       # (Windows) venv\Scripts\activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt


4ï¸âƒ£ Install and start Ollama
Download from ollama.ai/download
Pull a model (for example, Mistral):
ollama pull mistral


â–¶ï¸ Run the App
streamlit run local_bot.py
Then open your browser at http://localhost:8501

ğŸ”’ Privacy
Runs completely offline with Ollama â€” no data leaves your device.

